"""
è‡ªç„¶è¯­è¨€å¤„ç†å™¨ v2.0
Sprint 1 ç”Ÿäº§ç‰ˆæœ¬ - é›†æˆOpenAI APIå’Œæœ¬åœ°å¤‡ç”¨æ–¹æ¡ˆ
"""
import re
import json
import logging
import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime
import aiohttp
from openai import AsyncOpenAI

from models import QueryIntent, QueryType, QueryError
from config import config

logger = logging.getLogger(__name__)


class NLPProcessorV2:
    """ç”Ÿäº§çº§è‡ªç„¶è¯­è¨€å¤„ç†å™¨"""
    
    def __init__(self):
        self.openai_client = AsyncOpenAI(api_key=config.openai_api_key)
        self.api_call_count = 0
        self.api_cost_tracking = []
        self.query_patterns = self._load_query_patterns()
        self.fallback_enabled = True
        
    def _load_query_patterns(self) -> Dict[str, List[str]]:
        """åŠ è½½ä¼˜åŒ–çš„æŸ¥è¯¢æ¨¡å¼åŒ¹é…è§„åˆ™"""
        return {
            "trend": [
                r"è¶‹åŠ¿|å˜åŒ–|å¢é•¿|ä¸‹é™|èµ°åŠ¿|å‘å±•|æ¼”å˜|æ³¢åŠ¨|èµ·ä¼",
                r"è¿‡å».*?[æœˆå¤©å¹´]|æœ€è¿‘.*?[æœˆå¤©å¹´]|.*?æœŸé—´|å†å²|æ—¶é—´|å˜è¿",
                r"å¢é•¿ç‡|å˜åŒ–ç‡|åŒæ¯”|ç¯æ¯”|æ¶¨å¹…|è·Œå¹…|å¢é€Ÿ|é™é€Ÿ",
                r"è¿ç»­|æŒç»­|é€å¹´|é€æœˆ|é€æ—¥|å¹´åº¦|æœˆåº¦|æ—¥åº¦"
            ],
            "comparison": [
                r"å¯¹æ¯”|æ¯”è¾ƒ|ç›¸æ¯”|vs|versus|å¯¹ç…§|PK|æ¯”æ‹¼",
                r"å“ªä¸ª.*?[å¥½é«˜å¤šå¤§å°ä¼˜åŠ£å¼ºå¼±]|è°.*?[å¥½é«˜å¤šå¤§å°ä¼˜åŠ£å¼ºå¼±]",
                r"å·®å¼‚|åŒºåˆ«|å·®åˆ«|ä¸åŒ|ä¼˜åŠ£|å¼ºå¼±|é«˜ä½",
                r"å„.*?ä¹‹é—´|.*?å’Œ.*?|.*?ä¸.*?|.*?è·Ÿ.*?"
            ],
            "ranking": [
                r"æ’å|æ’è¡Œ|TOP|å‰.*?å|æœ€.*?çš„|ç¬¬.*?å|ä½åˆ—",
                r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1-9]|æœ€[é«˜ä½å¤§å°å¤šå°‘å¥½åä¼˜åŠ£å¼ºå¼±]",
                r"å† å†›|äºšå†›|å­£å†›|æ¦œé¦–|æœ«ä½|é¦–ä½|å€’æ•°",
                r"æ’åº|æ’åˆ—|åæ¬¡|åº§æ¬¡|ä½æ¬¡"
            ],
            "statistics": [
                r"æ€».*?|å¹³å‡|å¹³å‡å€¼|æ€»å’Œ|æ€»è®¡|åˆè®¡|æ±‡æ€»",
                r"ç»Ÿè®¡|æ±‡æ€»|æ¦‚å†µ|æƒ…å†µ|æ•°æ®|æŒ‡æ ‡|æ¦‚è§ˆ",
                r"æ•°é‡|ä¸ªæ•°|æ€»æ•°|è®¡æ•°|æ±‚å’Œ|ç´¯è®¡",
                r"æœ€å¤§å€¼|æœ€å°å€¼|æœ€é«˜|æœ€ä½|æå€¼|å‡å€¼"
                r"å¤šå°‘|å‡ ä¸ª|æ•°é‡|æ€»æ•°"
            ],
            "proportion": [
                r"å æ¯”|æ¯”ä¾‹|ç™¾åˆ†æ¯”|ä»½é¢|å æ®",
                r"åˆ†å¸ƒ|æ„æˆ|ç»„æˆ|æ¯”é‡|æƒé‡",
                r"%|percent|æ¯”ç‡"
            ]
        }
    
    async def parse_query_intent(self, natural_query: str, 
                                use_openai: bool = True) -> QueryIntent:
        """
        è§£ææŸ¥è¯¢æ„å›¾ - ç”Ÿäº§çº§å®ç°
        
        Args:
            natural_query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            use_openai: æ˜¯å¦ä½¿ç”¨OpenAI API
            
        Returns:
            QueryIntent: è§£æåçš„æŸ¥è¯¢æ„å›¾
        """
        try:
            # è®°å½•APIè°ƒç”¨
            start_time = datetime.now()
            
            # é¢„å¤„ç†æŸ¥è¯¢
            cleaned_query = self._preprocess_query(natural_query)
            logger.info(f"ğŸ§  å¤„ç†æŸ¥è¯¢: {cleaned_query}")
            
            # åŸºç¡€æ¨¡å¼åŒ¹é…
            base_type = self._pattern_classify(cleaned_query)
            base_confidence = 0.6
            
            # å¦‚æœå¯ç”¨OpenAIä¸”é…ç½®æ­£ç¡®ï¼Œä½¿ç”¨æ·±åº¦è§£æ
            if use_openai and self._should_use_openai():
                try:
                    intent_result = await self._openai_parse_intent(
                        cleaned_query, base_type
                    )
                    self._track_api_usage(start_time)
                    return intent_result
                    
                except Exception as e:
                    logger.warning(f"OpenAIè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {e}")
                    # é™çº§åˆ°æœ¬åœ°è§£æ
                    
            # æœ¬åœ°å¤‡ç”¨è§£ææ–¹æ¡ˆ
            return self._local_parse_intent(cleaned_query, base_type, base_confidence)
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ„å›¾è§£æå¤±è´¥: {str(e)}")
            raise QueryError(
                error_type="INTENT_PARSING_ERROR",
                error_message=str(e),
                user_friendly_message="æ— æ³•ç†è§£æ‚¨çš„æŸ¥è¯¢ï¼Œè¯·å°è¯•ç”¨æ›´ç®€å•çš„è¯­è¨€æè¿°ã€‚",
                suggestions=[
                    "è¯·ä½¿ç”¨æ›´ç®€å•ç›´æ¥çš„è¯­è¨€",
                    "æ˜ç¡®æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œå¦‚'è¿‡å»3ä¸ªæœˆ'",
                    "æ˜ç¡®æŒ‡å®šåˆ†æå¯¹è±¡ï¼Œå¦‚'é”€å”®é¢'ã€'ç”¨æˆ·æ•°'"
                ]
            )
    
    def _should_use_openai(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨OpenAI API"""
        # æ£€æŸ¥æ¯æ—¥è°ƒç”¨é™åˆ¶
        daily_limit = 1000  # æ¯æ—¥é™åˆ¶1000æ¬¡è°ƒç”¨
        if self.api_call_count >= daily_limit:
            logger.warning("å·²è¾¾åˆ°æ¯æ—¥OpenAI APIè°ƒç”¨é™åˆ¶")
            return False
        
        # æ£€æŸ¥APIå¯†é’¥
        if not config.openai_api_key or config.openai_api_key == "test_key_placeholder":
            logger.warning("OpenAI APIå¯†é’¥æœªé…ç½®")
            return False
        
        return True
    
    async def _openai_parse_intent(self, query: str, base_type: QueryType) -> QueryIntent:
        """ä½¿ç”¨OpenAIè¿›è¡Œæ·±åº¦è§£æ"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æ•°æ®æŸ¥è¯¢æ„å›¾è§£æä¸“å®¶ã€‚

ç”¨æˆ·ä¼šç”¨è‡ªç„¶è¯­è¨€æè¿°æ•°æ®åˆ†æéœ€æ±‚ï¼Œè¯·å‡†ç¡®è§£æï¼š
1. æŸ¥è¯¢ç±»å‹ï¼štrend(è¶‹åŠ¿åˆ†æ)ã€comparison(å¯¹æ¯”åˆ†æ)ã€ranking(æ’ååˆ†æ)ã€statistics(ç»Ÿè®¡åˆ†æ)ã€proportion(å æ¯”åˆ†æ)
2. å®ä½“è¯ï¼šæ¶‰åŠçš„ä¸šåŠ¡å¯¹è±¡(äº§å“ã€éƒ¨é—¨ã€å®¢æˆ·ç­‰)
3. æ—¶é—´èŒƒå›´ï¼šæŸ¥è¯¢çš„æ—¶é—´èŒƒå›´
4. åˆ†æç»´åº¦ï¼šæŒ‰ä»€ä¹ˆç»´åº¦åˆ†ç»„
5. åˆ†ææŒ‡æ ‡ï¼šå…³æ³¨çš„æ•°æ®æŒ‡æ ‡
6. ç­›é€‰æ¡ä»¶ï¼šæŸ¥è¯¢çš„è¿‡æ»¤æ¡ä»¶
7. ç½®ä¿¡åº¦ï¼šå¯¹è§£æç»“æœçš„ç¡®ä¿¡ç¨‹åº¦(0-1)

ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¿”å›ï¼š
{
  "query_type": "trend|comparison|ranking|statistics|proportion",
  "entities": ["å®ä½“1", "å®ä½“2"],
  "time_range": "æ—¶é—´èŒƒå›´",
  "dimensions": ["ç»´åº¦1"],
  "metrics": ["æŒ‡æ ‡1"],
  "filters": {},
  "confidence": 0.95
}"""

        user_prompt = f"""è¯·è§£ææŸ¥è¯¢: "{query}"

åˆæ­¥åˆ†ç±»: {base_type.value}

è¿”å›æ ‡å‡†JSONæ ¼å¼ç»“æœã€‚"""

        try:
            response = await self.openai_client.chat.completions.create(
                model=config.openai_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=config.openai_temperature,
                max_tokens=config.openai_max_tokens,
                timeout=10.0  # 10ç§’è¶…æ—¶
            )
            
            content = response.choices[0].message.content.strip()
            
            # æå–JSON
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                intent_data = json.loads(json_str)
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if self._validate_intent_data(intent_data):
                    return QueryIntent(**intent_data)
                else:
                    raise ValueError("OpenAIè¿”å›æ•°æ®ä¸å®Œæ•´")
            else:
                raise ValueError("OpenAIè¿”å›æ ¼å¼ä¸æ­£ç¡®")
                
        except asyncio.TimeoutError:
            logger.error("OpenAI APIè°ƒç”¨è¶…æ—¶")
            raise
        except Exception as e:
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _local_parse_intent(self, query: str, base_type: QueryType, 
                           confidence: float) -> QueryIntent:
        """æœ¬åœ°å¤‡ç”¨è§£ææ–¹æ¡ˆ - å¢å¼ºç½®ä¿¡åº¦è¯„ä¼°"""
        logger.info("ğŸ”„ ä½¿ç”¨æœ¬åœ°è§£ææ–¹æ¡ˆ")
        
        # å®ä½“è¯†åˆ«
        entities = self._extract_entities(query)
        
        # æ—¶é—´èŒƒå›´è¯†åˆ«
        time_range = self._extract_time_range(query)
        
        # ç»´åº¦å’ŒæŒ‡æ ‡è¯†åˆ«
        dimensions, metrics = self._extract_dimensions_metrics(query)
        
        # ç­›é€‰æ¡ä»¶è¯†åˆ«
        filters = self._extract_filters(query)
        
        # è®¡ç®—å¢å¼ºçš„ç½®ä¿¡åº¦
        enhanced_confidence = self._calculate_enhanced_confidence(
            query, base_type, entities, time_range, dimensions, metrics
        )
        
        return QueryIntent(
            query_type=base_type,
            entities=entities,
            time_range=time_range,
            dimensions=dimensions,
            metrics=metrics,
            filters=filters,
            confidence=enhanced_confidence
        )
    
    def _calculate_enhanced_confidence(self, query: str, query_type: QueryType, 
                                     entities: List[str], time_range: Optional[str],
                                     dimensions: List[str], metrics: List[str]) -> float:
        """
        è®¡ç®—å¢å¼ºçš„ç½®ä¿¡åº¦è¯„åˆ†
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            query_type: æŸ¥è¯¢ç±»å‹
            entities: è¯†åˆ«çš„å®ä½“
            time_range: æ—¶é—´èŒƒå›´
            dimensions: åˆ†æç»´åº¦
            metrics: åˆ†ææŒ‡æ ‡
            
        Returns:
            float: ç½®ä¿¡åº¦è¯„åˆ† (0-1)
        """
        confidence_factors = []
        
        # 1. æŸ¥è¯¢ç±»å‹åŒ¹é…åº¦ (0.3æƒé‡)
        type_patterns = self.query_patterns.get(query_type.value, [])
        type_match_count = 0
        for pattern in type_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                type_match_count += 1
        
        type_confidence = min(type_match_count / len(type_patterns), 1.0) if type_patterns else 0.3
        confidence_factors.append(("query_type", type_confidence, 0.3))
        
        # 2. å®ä½“è¯†åˆ«å®Œæ•´æ€§ (0.2æƒé‡) 
        # æ ¹æ®å®ä½“æ•°é‡å’Œè´¨é‡è¯„åˆ†
        entity_score = 0
        if len(entities) >= 3:
            entity_score = 1.0
        elif len(entities) >= 2:
            entity_score = 0.8
        elif len(entities) >= 1:
            entity_score = 0.6
        else:
            entity_score = 0.3
        confidence_factors.append(("entities", entity_score, 0.2))
        
        # 3. æ—¶é—´èŒƒå›´æ˜ç¡®åº¦ (0.2æƒé‡)
        time_confidence = 0.9 if time_range else 0.4  # æœ‰æ˜ç¡®æ—¶é—´èŒƒå›´ç½®ä¿¡åº¦æ›´é«˜
        confidence_factors.append(("time_range", time_confidence, 0.2))
        
        # 4. åˆ†æç»´åº¦æ¸…æ™°åº¦ (0.15æƒé‡)
        dim_confidence = min(len(dimensions) / 1, 1.0) if dimensions else 0.5
        confidence_factors.append(("dimensions", dim_confidence, 0.15))
        
        # 5. åˆ†ææŒ‡æ ‡æ˜ç¡®æ€§ (0.15æƒé‡)
        metric_confidence = min(len(metrics) / 1, 1.0) if metrics else 0.5
        confidence_factors.append(("metrics", metric_confidence, 0.15))
        
        # è®¡ç®—åŠ æƒå¹³å‡ç½®ä¿¡åº¦
        weighted_confidence = sum(score * weight for _, score, weight in confidence_factors)
        
        # é¢å¤–åŠ åˆ†é¡¹
        bonus_points = 0
        
        # æŸ¥è¯¢é•¿åº¦åˆç†æ€§åŠ åˆ†
        query_length = len(query.strip())
        if 10 <= query_length <= 100:
            bonus_points += 0.05
        
        # å…³é”®è¯å¯†åº¦åŠ åˆ†
        business_keywords = [
            "é”€å”®", "ä¸šç»©", "æ•°æ®", "åˆ†æ", "ç»Ÿè®¡", "å¯¹æ¯”", "è¶‹åŠ¿",
            "è¥æ”¶", "åˆ©æ¶¦", "æˆæœ¬", "å®¢æˆ·", "äº§å“", "éƒ¨é—¨", "åœ°åŒº"
        ]
        keyword_count = sum(1 for kw in business_keywords if kw in query)
        if keyword_count >= 2:
            bonus_points += 0.05
        if keyword_count >= 4:
            bonus_points += 0.05  # å…³é”®è¯æ›´å¤šæ—¶é¢å¤–åŠ åˆ†
        
        # æœ€ç»ˆç½®ä¿¡åº¦
        final_confidence = min(weighted_confidence + bonus_points, 1.0)
        
        # è®°å½•ç½®ä¿¡åº¦è®¡ç®—è¯¦æƒ…
        logger.info(f"ğŸ“Š ç½®ä¿¡åº¦è®¡ç®—è¯¦æƒ…:")
        for factor, score, weight in confidence_factors:
            logger.info(f"  - {factor}: {score:.2f} (æƒé‡: {weight})")
        logger.info(f"  - åŠ åˆ†é¡¹: {bonus_points:.2f}")
        logger.info(f"  - æœ€ç»ˆç½®ä¿¡åº¦: {final_confidence:.2f}")
        
        return round(final_confidence, 2)
    
    def _preprocess_query(self, query: str) -> str:
        """æŸ¥è¯¢é¢„å¤„ç†ä¼˜åŒ–"""
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        query = re.sub(r'\s+', ' ', query.strip())
        query = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', '', query)
        
        # æ ‡å‡†åŒ–æ—¶é—´è¡¨è¾¾
        time_mappings = {
            "ä»Šå¤©": "today", "æ˜¨å¤©": "yesterday", "æ˜å¤©": "tomorrow",
            "æœ¬å‘¨": "this week", "ä¸Šå‘¨": "last week", "ä¸‹å‘¨": "next week",
            "æœ¬æœˆ": "this month", "ä¸Šæœˆ": "last month", "ä¸‹æœˆ": "next month",
            "ä»Šå¹´": "this year", "å»å¹´": "last year", "æ˜å¹´": "next year",
            "æœ€è¿‘": "recent", "è¿‡å»": "past"
        }
        
        for chinese, english in time_mappings.items():
            query = query.replace(chinese, english)
        
        return query
    
    def _pattern_classify(self, query: str) -> QueryType:
        """ä¼˜åŒ–çš„æ¨¡å¼åˆ†ç±»"""
        type_scores = {}
        
        for query_type, patterns in self.query_patterns.items():
            score = 0
            for pattern in patterns:
                matches = re.findall(pattern, query, re.IGNORECASE)
                score += len(matches)
            type_scores[query_type] = score
        
        # è·å–æœ€é«˜åˆ†ç±»å‹
        if type_scores:
            best_type = max(type_scores.items(), key=lambda x: x[1])
            if best_type[1] > 0:
                return QueryType(best_type[0])
        
        # é»˜è®¤ä¸ºç»Ÿè®¡åˆ†æ
        return QueryType.STATISTICS
    
    def _extract_entities(self, query: str) -> List[str]:
        """å®ä½“è¯†åˆ«"""
        entities = []
        
        # å¢å¼ºçš„ä¸šåŠ¡å®ä½“å…³é”®è¯åº“
        business_entities = {
            # ä¸šåŠ¡å¯¹è±¡
            "éƒ¨é—¨": ["éƒ¨é—¨", "ç§‘å®¤", "åˆ†å…¬å¸", "äº‹ä¸šéƒ¨", "ä¸­å¿ƒ"],
            "äº§å“": ["äº§å“", "å•†å“", "æœåŠ¡", "é¡¹ç›®", "æ–¹æ¡ˆ"],
            "å®¢æˆ·": ["å®¢æˆ·", "ç”¨æˆ·", "æ¶ˆè´¹è€…", "ä¹°å®¶", "é¡¾å®¢"],
            "å‘˜å·¥": ["å‘˜å·¥", "äººå‘˜", "èŒå‘˜", "å·¥ä½œäººå‘˜", "å›¢é˜Ÿ"],
            "åœ°åŒº": ["åœ°åŒº", "åŒºåŸŸ", "åŸå¸‚", "çœä»½", "å¸‚åœº"]
        }
        
        # ä¸šåŠ¡æŒ‡æ ‡å…³é”®è¯
        business_metrics = [
            "é”€å”®é¢", "è¥ä¸šé¢", "æ”¶å…¥", "åˆ©æ¶¦", "æˆæœ¬", "è´¹ç”¨",
            "ä¸šç»©", "ç»©æ•ˆ", "å¢é•¿ç‡", "å æ¯”", "ä»½é¢", "æ•°é‡",
            "å•ä»·", "å®¢å•ä»·", "è½¬åŒ–ç‡", "ç•™å­˜ç‡", "æ´»è·ƒåº¦"
        ]
        
        # è¯†åˆ«ä¸šåŠ¡å¯¹è±¡
        for category, keywords in business_entities.items():
            for keyword in keywords:
                if keyword in query:
                    entities.append(keyword)
                    break  # æ¯ä¸ªç±»åˆ«åªæ·»åŠ ä¸€æ¬¡
        
        # è¯†åˆ«ä¸šåŠ¡æŒ‡æ ‡
        for metric in business_metrics:
            if metric in query:
                entities.append(metric)
        
        return list(set(entities))  # å»é‡
    
    def _extract_time_range(self, query: str) -> Optional[str]:
        """æ—¶é—´èŒƒå›´æå–"""
        time_patterns = [
            r"past \d+ month", r"last \d+ month", r"recent \d+ month",
            r"today", r"yesterday", r"this week", r"last week",
            r"this month", r"last month", r"this year", r"last year"
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                return match.group()
        
        return None
    
    def _extract_dimensions_metrics(self, query: str) -> tuple:
        """ç»´åº¦å’ŒæŒ‡æ ‡æå–"""
        dimensions = []
        metrics = []
        
        # å¸¸è§ç»´åº¦
        dimension_words = ["éƒ¨é—¨", "åœ°åŒº", "äº§å“", "æ¸ é“", "å®¢æˆ·", "æ—¶é—´"]
        for dim in dimension_words:
            if dim in query:
                dimensions.append(dim)
        
        # å¸¸è§æŒ‡æ ‡
        metric_words = ["é”€å”®é¢", "æ”¶å…¥", "åˆ©æ¶¦", "æ•°é‡", "å®¢æˆ·æ•°", "ç”¨æˆ·æ•°"]
        for metric in metric_words:
            if metric in query:
                metrics.append(metric)
        
        return dimensions, metrics
    
    def _extract_filters(self, query: str) -> Dict[str, Any]:
        """ç­›é€‰æ¡ä»¶æå–"""
        filters = {}
        
        # ç®€å•çš„æ¡ä»¶è¯†åˆ«
        if "å¤§äº" in query or "è¶…è¿‡" in query:
            filters["condition"] = "greater_than"
        elif "å°äº" in query or "ä½äº" in query:
            filters["condition"] = "less_than"
        elif "ç­‰äº" in query:
            filters["condition"] = "equal"
        
        return filters
    
    def _validate_intent_data(self, data: Dict[str, Any]) -> bool:
        """éªŒè¯æ„å›¾æ•°æ®å®Œæ•´æ€§"""
        required_fields = ["query_type", "entities", "confidence"]
        return all(field in data for field in required_fields)
    
    def _track_api_usage(self, start_time: datetime):
        """è·Ÿè¸ªAPIä½¿ç”¨æƒ…å†µ"""
        self.api_call_count += 1
        duration = (datetime.now() - start_time).total_seconds()
        
        self.api_cost_tracking.append({
            "timestamp": start_time.isoformat(),
            "duration": duration,
            "call_count": self.api_call_count
        })
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š APIè°ƒç”¨ç»Ÿè®¡: ç¬¬{self.api_call_count}æ¬¡, è€—æ—¶{duration:.2f}ç§’")
    
    def get_api_statistics(self) -> Dict[str, Any]:
        """è·å–APIä½¿ç”¨ç»Ÿè®¡"""
        return {
            "total_calls": self.api_call_count,
            "daily_limit": 1000,
            "remaining_calls": max(0, 1000 - self.api_call_count),
            "recent_calls": self.api_cost_tracking[-10:] if self.api_cost_tracking else []
        }